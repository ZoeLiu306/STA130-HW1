{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3480af41",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/ryanrossi/datasets/master/dog-breeds/dog-breeds.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ryanrossi/datasets/master/dog-breeds/dog-breeds.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84120d",
   "metadata": {},
   "source": [
    "Summary of Interaction\n",
    "\n",
    "In our interaction, you requested help finding an amusing, funny, or otherwise interesting dataset that has missing values and is available online through a URL link to a CSV file. I provided several options, including datasets related to \"The Simpsons,\" \"Jeopardy!\", \"The Office,\" and wine reviews. You specifically asked for a dataset that could be imported directly into Python. After reviewing the options, you requested the URL for the Wine Reviews Dataset, which I provided, along with sample Python code to import it using the `pandas` library. Finally, you asked for a summary of our interaction and the final version of the Python code we created.\n",
    "\n",
    "Final Working Version of the Code\n",
    "\n",
    "Here is the final version of the Python code that you can use to import and explore the Wine Reviews Dataset:\n",
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/cognitiveclass/DSX-Workshop/master/wine-reviews/winemag-data_first150k.csv\"\n",
    "wine_data = pd.read_csv(url)\n",
    "\n",
    "print(wine_data.head())\n",
    "\n",
    "This code imports the Wine Reviews Dataset from the provided URL and displays the first few rows of the data. You can modify it further depending on your analysis or assignment requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28efc346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived  Pclass                                               Name  \\\n",
      "0         0       3                             Mr. Owen Harris Braund   \n",
      "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
      "2         1       3                              Miss. Laina Heikkinen   \n",
      "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
      "4         0       3                            Mr. William Henry Allen   \n",
      "\n",
      "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
      "0    male  22.0                        1                        0   7.2500  \n",
      "1  female  38.0                        1                        0  71.2833  \n",
      "2  female  26.0                        0                        0   7.9250  \n",
      "3  female  35.0                        1                        0  53.1000  \n",
      "4    male  35.0                        0                        0   8.0500  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a9fd44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived                   False\n",
      "Pclass                     False\n",
      "Name                       False\n",
      "Sex                        False\n",
      "Age                        False\n",
      "Siblings/Spouses Aboard    False\n",
      "Parents/Children Aboard    False\n",
      "Fare                       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "any_missing_values = data.isnull().any()\n",
    "print(any_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e6c5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived                   0\n",
       "Pclass                     0\n",
       "Name                       0\n",
       "Sex                        0\n",
       "Age                        0\n",
       "Siblings/Spouses Aboard    0\n",
       "Parents/Children Aboard    0\n",
       "Fare                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22db065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 887 rows and 8 columns.\n"
     ]
    }
   ],
   "source": [
    "rows, columns = data.shape\n",
    "print(f\"The dataset has {rows} rows and {columns} columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30b27d",
   "metadata": {},
   "source": [
    "The chatbot give me the definition of \"Observations\" and \"variables\" are like:\n",
    "\n",
    "1. Observations (Rows)\n",
    "Definition: An observation refers to a single instance or entity for which data has been collected. In the Titanic dataset, each observation typically represents one passenger on the Titanic.\n",
    "\n",
    "In the Titanic dataset: Each row is an observation. It could represent a single passenger, detailing characteristics like their name, age, gender, whether they survived, etc.\n",
    "\n",
    "2. Variables (Columns)\n",
    "Definition: A variable is a characteristic or attribute that is measured or recorded for each observation. Each variable corresponds to a column in a dataset.\n",
    "\n",
    "In the Titanic dataset: Examples of variables include \"PassengerId,\" \"Survived,\" \"Pclass,\" \"Age,\" \"Fare,\" and more. Each of these represents a specific type of information collected for each passenger.\n",
    "\n",
    "My Summary:\n",
    "\"Observations\" are the individual passengers, which are rows of my datalist\n",
    "\"Variables\" are other features like age, fare, which are columes of the datalist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc4d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Survived      Pclass         Age  Siblings/Spouses Aboard  \\\n",
      "count  887.000000  887.000000  887.000000               887.000000   \n",
      "mean     0.385569    2.305524   29.471443                 0.525366   \n",
      "std      0.487004    0.836662   14.121908                 1.104669   \n",
      "min      0.000000    1.000000    0.420000                 0.000000   \n",
      "25%      0.000000    2.000000   20.250000                 0.000000   \n",
      "50%      0.000000    3.000000   28.000000                 0.000000   \n",
      "75%      1.000000    3.000000   38.000000                 1.000000   \n",
      "max      1.000000    3.000000   80.000000                 8.000000   \n",
      "\n",
      "       Parents/Children Aboard       Fare  \n",
      "count               887.000000  887.00000  \n",
      "mean                  0.383315   32.30542  \n",
      "std                   0.807466   49.78204  \n",
      "min                   0.000000    0.00000  \n",
      "25%                   0.000000    7.92500  \n",
      "50%                   0.000000   14.45420  \n",
      "75%                   0.000000   31.13750  \n",
      "max                   6.000000  512.32920  \n"
     ]
    }
   ],
   "source": [
    "summary_numeric = data.describe()\n",
    "print(summary_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7bfec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 887 entries, 0 to 886\n",
      "Data columns (total 8 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Survived                 887 non-null    int64  \n",
      " 1   Pclass                   887 non-null    int64  \n",
      " 2   Name                     887 non-null    object \n",
      " 3   Sex                      887 non-null    object \n",
      " 4   Age                      887 non-null    float64\n",
      " 5   Siblings/Spouses Aboard  887 non-null    int64  \n",
      " 6   Parents/Children Aboard  887 non-null    int64  \n",
      " 7   Fare                     887 non-null    float64\n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 55.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "summary_info = data.info()\n",
    "print(summary_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53fe891e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Name   Sex\n",
      "count                      887   887\n",
      "unique                     887     2\n",
      "top     Mr. Owen Harris Braund  male\n",
      "freq                         1   573\n"
     ]
    }
   ],
   "source": [
    "summary_categorical = data.describe(include='object')\n",
    "print(summary_categorical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba0b09c",
   "metadata": {},
   "source": [
    "Chatbot give me the diffenence between\n",
    "    an \"attribute\", such as df.shape which does not end with ()\n",
    "    and a \"method\", such as df.describe() which does end with ()\n",
    "   \n",
    "Here's my summary:\n",
    "1.Attributes are similar to those that provide information about a DataFrame, and no parentheses\n",
    "2.Methods are functions that perform an action or return a result, which require parentheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa52b8a",
   "metadata": {},
   "source": [
    "For df.describe() method, here are definitions of each variables it analyzes. \n",
    "'count':are the number of non-null entries.\n",
    "\n",
    "'mean':is the average value\n",
    "\n",
    "'std':Standard deviation\n",
    "\n",
    "'min':minimum value\n",
    "\n",
    "'25%':quartile 1, percentage\n",
    "\n",
    "'50%':quartile 2, medium, percentage\n",
    "\n",
    "'75%':quartile 3, percentage\n",
    "\n",
    "'max':maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39ca0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d12f293d",
   "metadata": {},
   "source": [
    "For question number 7.\n",
    "df.dropna() or del df['col']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ff55f0",
   "metadata": {},
   "source": [
    "1.If there's a datalist of sales，I want to delete the last colume which dosen't provide me any useful data, then I'll use del df['col']. \n",
    "\n",
    "2.If I want to delete one row with some missing data, then I'll use df.dropna(). \n",
    "\n",
    "3.To discuss from Memory Efficiency, Data Cleaning Order and Operational Focus, applying del df['col'] before df.dropna() when both are used together could be important. Using del df['col'] can remove useless data first, can reduce the memory cost by additional DataFrame. And it's more efficent to remove those irrelvant colume, it can make the operation faster. Also, remove the useless data can make you better focus on the useful data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "869bc223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name   Age           City   Salary\n",
      "0    Alice  25.0       New York  70000.0\n",
      "1      Bob   NaN    Los Angeles  80000.0\n",
      "2  Charlie  30.0        Chicago      NaN\n",
      "3    David  22.0           None  60000.0\n",
      "4      Eve   NaN  San Francisco  75000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with some missing data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, None, 30, 22, None],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', None, 'San Francisco'],\n",
    "    'Salary': [70000, 80000, None, 60000, 75000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e26aed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing 'City' column:\n",
      "      Name   Age           City   Salary\n",
      "0    Alice  25.0       New York  70000.0\n",
      "1      Bob   NaN    Los Angeles  80000.0\n",
      "2  Charlie  30.0        Chicago      NaN\n",
      "3    David  22.0           None  60000.0\n",
      "4      Eve   NaN  San Francisco  75000.0\n",
      "\n",
      "After removing 'City' column:\n",
      "      Name   Age   Salary\n",
      "0    Alice  25.0  70000.0\n",
      "1      Bob   NaN  80000.0\n",
      "2  Charlie  30.0      NaN\n",
      "3    David  22.0  60000.0\n",
      "4      Eve   NaN  75000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with some missing data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, None, 30, 22, None],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', None, 'San Francisco'],\n",
    "    'Salary': [70000, 80000, None, 60000, 75000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame before removing the column\n",
    "print(\"Before removing 'City' column:\")\n",
    "print(df)\n",
    "\n",
    "# Remove the 'City' column\n",
    "del df['City']\n",
    "\n",
    "# Print the DataFrame after removing the column\n",
    "print(\"\\nAfter removing 'City' column:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a166a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping rows with missing data:\n",
      "      Name   Age           City   Salary\n",
      "0    Alice  25.0       New York  70000.0\n",
      "1      Bob   NaN    Los Angeles  80000.0\n",
      "2  Charlie  30.0        Chicago      NaN\n",
      "3    David  22.0           None  60000.0\n",
      "4      Eve   NaN  San Francisco  75000.0\n",
      "\n",
      "After dropping rows with missing data:\n",
      "    Name   Age      City   Salary\n",
      "0  Alice  25.0  New York  70000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with some missing data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'Age': [25, None, 30, 22, None],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', None, 'San Francisco'],\n",
    "    'Salary': [70000, 80000, None, 60000, 75000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame before dropping rows with missing data\n",
    "print(\"Before dropping rows with missing data:\")\n",
    "print(df)\n",
    "\n",
    "# Remove rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Print the DataFrame after dropping rows with missing data\n",
    "print(\"\\nAfter dropping rows with missing data:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa6078",
   "metadata": {},
   "source": [
    "So, for number 4. \n",
    "I first use del df['col'] to remove the column \"city\", the data then only contain useful data. Later I use df.dropna() to delete all missing data, and the data only include one row that have no missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148ff5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c689f72f",
   "metadata": {},
   "source": [
    "8.1: Use your ChatBot session to understand what df.groupby(\"col1\")[\"col2\"].describe() does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you\n",
    "\n",
    "Explanation\n",
    "    df.groupby(\"col1\"): Groups the DataFrame by the unique values in col1.\n",
    "    [\"col2\"]: Selects the column col2 for which you want to compute statistics.\n",
    "    .describe(): Provides summary statistics for col2 within each group defined by col1\n",
    "\n",
    "Grouping: The DataFrame is grouped by Pclass (Passenger Class).\n",
    "Aggregation: For each class, the Age column is summarized:\n",
    "Pclass 1 (First Class):\n",
    "\n",
    "count: 216 entries\n",
    "mean: Average age is approximately 38.2 years\n",
    "std: Standard deviation is 14.3 years\n",
    "min: Minimum age is 22 years\n",
    "25%: 25th percentile is 30 years\n",
    "50%: Median (50th percentile) is 37 years\n",
    "75%: 75th percentile is 44 years\n",
    "max: Maximum age is 80 years\n",
    "Pclass 2 (Second Class):\n",
    "\n",
    "count: 184 entries\n",
    "mean: Average age is approximately 29.9 years\n",
    "std: Standard deviation is 14.4 years\n",
    "min: Minimum age is 20 years\n",
    "25%: 25th percentile is 22 years\n",
    "50%: Median (50th percentile) is 28 years\n",
    "75%: 75th percentile is 35 years\n",
    "max: Maximum age is 60 years\n",
    "Pclass 3 (Third Class):\n",
    "\n",
    "count: 491 entries\n",
    "mean: Average age is approximately 25.1 years\n",
    "std: Standard deviation is 15.2 years\n",
    "min: Minimum age is 0.42 years (infant)\n",
    "25%: 25th percentile is 16 years\n",
    "50%: Median (50th percentile) is 24 years\n",
    "75%: 75th percentile is 32 years\n",
    "max: Maximum age is 80 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff9ba0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count       mean        std   min    25%   50%     75%   max\n",
      "Pclass                                                              \n",
      "1       216.0  38.788981  14.183633  0.92  28.75  38.5  48.000  80.0\n",
      "2       184.0  29.868641  13.756191  0.67  23.00  29.0  36.625  70.0\n",
      "3       487.0  25.188747  12.095084  0.42  19.00  24.0  31.000  74.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Group by 'Pclass' and describe 'Age'\n",
    "age_description_by_pclass = df.groupby('Pclass')['Age'].describe()\n",
    "\n",
    "print(age_description_by_pclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba343eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeb8b3e4",
   "metadata": {},
   "source": [
    "8.2:Assuming you've not yet removed missing values in the manner of question \"7\" above, df.describe() would have different values in the count value for different data columns depending on the missingness present in the original data. Why do these capture something fundamentally different from the values in the count that result from doing something like df.groupby(\"col1\")[\"col2\"].describe()?\n",
    "\n",
    "df.describe() Count\n",
    "Purpose: The count in df.describe() provides the number of non-null entries for each column in the entire DataFrame.\n",
    "\n",
    "df.groupby(\"col1\")[\"col2\"].describe()\n",
    "\n",
    "Purpose: The count in df.groupby(\"col1\")[\"col2\"].describe() provides the number of non-null entries for col2 within each group defined by col1.\n",
    "Scope: It is specific to each group created by the groupby operation, giving insights into how many non-null values exist for col2 in each group.\n",
    "Behavior with Missing Data: Each group might have a different number of non-null entries depending on the presence of missing values in col2 within that specific group.\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "1.Scope of Counts:\n",
    "df.describe(): The count reflects non-null values for each column in the entire DataFrame. It does not distinguish between different categories or groups.\n",
    "df.groupby(\"col1\")[\"col2\"].describe(): The count reflects non-null values for col2 within each group defined by col1. Each group can have a different number of non-null values.\n",
    "\n",
    "2.Effect of Missing Values:\n",
    "In df.describe(), missing values across the entire DataFrame reduce the count for columns with missing data.\n",
    "In df.groupby(\"col1\")[\"col2\"].describe(), the count is computed separately for each group. Missing values in col2 affect only the count within the specific group, not globally.\n",
    "\n",
    "3.Insights:\n",
    "df.describe(): Provides a high-level overview of the entire dataset’s completeness and summary statistics.\n",
    "df.groupby(\"col1\")[\"col2\"].describe(): Offers insights into the completeness and summary statistics of col2 within each group, showing how missing data varies across different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb4b450c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        count  mean      std   min    25%   50%    75%   max\n",
      "Pclass                                                      \n",
      "1         2.0  23.5  2.12132  22.0  22.75  23.5  24.25  25.0\n",
      "2         1.0  30.0      NaN  30.0  30.00  30.0  30.00  30.0\n",
      "3         0.0   NaN      NaN   NaN    NaN   NaN    NaN   NaN\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame with missing values\n",
    "data = {\n",
    "    'Pclass': [1, 2, 2, 1, 3],\n",
    "    'Age': [25, None, 30, 22, None]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Group by 'Pclass' and describe 'Age'\n",
    "grouped_description = df.groupby('Pclass')['Age'].describe()\n",
    "\n",
    "print(grouped_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc38ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192f6dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0731c2c2",
   "metadata": {},
   "source": [
    "Question 8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf9775e",
   "metadata": {},
   "source": [
    "See other notbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ea55e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40f3688",
   "metadata": {},
   "source": [
    "Question 9 \n",
    "I have fully interact with chatbot, it give me the correct answer really efficient, and most of the answers fix my prolems. I feel like I'll continue using chatbot when facing some coding errors, it is the most efficient way for me to work on my project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481ba39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
